{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ad1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE YOU START: pip install langchain langchain-google-vertexai chromadb\n",
    "\n",
    "from langchain_google_vertexai import GemmaVertexAIModelGarden\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Set up your Gemini API credentials (service account or API key)\n",
    "# Example for service account credentials:\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/path/to/your/service-account.json'\n",
    "\n",
    "# 1. Load and split documents\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "documents = TextLoader('your_docs_folder/').load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# 2. Embed and index docs (using Google Palm embeddings)\n",
    "embeddings = GooglePalmEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# 3. Set up Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 4. Initialize Gemma via Vertex AI\n",
    "llm = GemmaVertexAIModelGarden(\n",
    "    endpoint_id=\"YOUR_VERTEX_AI_GEMMA_ENDPOINT_ID\",\n",
    "    project=\"YOUR_GCP_PROJECT\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# 5. Build RAG Pipeline\n",
    "prompt_template = \"\"\"Answer the following question based on the context:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 6. Ask a Question w/RAG\n",
    "query = \"What is retrieval augmented generation?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566a91cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am. I process the text you send me.\n",
      "\n",
      "How can I help you, or what would you like to discuss?\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"are you listening to me?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecfb1e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: Models.generate_content() got an unexpected keyword argument 'generation_config'\n",
      "Summarized Text:\n",
      "An unexpected error occurred during summarization.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7d5b82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:480: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:480: SyntaxWarning: invalid escape sequence '\\`'\n",
      "/tmp/ipykernel_226843/1484244037.py:480: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  <Your summary here> \\```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      "```\n",
      "```summary\n",
      "The text discusses the availability of Google's Gemma 27B model through APIs, both free and paid. Here's a breakdown:\n",
      "\n",
      "**API Access Options:**\n",
      "\n",
      "*   **Official Google APIs (Gemini API, Vertex AI):** Require API keys and are subject to billing beyond initial trial credits.\n",
      "*   **Third-Party Gateways (OpenRouter, LangDB):** Offer free access under usage-limited tiers.\n",
      "*   **Community-Hosted Endpoints (Together AI, DeepInfra, Hugging Face):** Primarily paid, with limited or no free tiers.\n",
      "*   **Self-Hosting:** Downloading the model weights allows for cost-predictable usage (infrastructure costs only) but requires significant resources.\n",
      "\n",
      "**Gemma API Integration with LangChain:**\n",
      "\n",
      "*   Requires a Google Cloud project with the Generative AI API enabled and a valid API key or service account.\n",
      "*   LangChain integration involves installing necessary packages and configuring credentials.\n",
      "*   The latest version of LangChain uses the standalone `google-generativeai` package instead of the deprecated `langchain-google-genai` bridge.\n",
      "\n",
      "**Text Summarization with JSON Schema:**\n",
      "\n",
      "*   The text demonstrates how to instruct Gemma to return summaries in JSON format using a schema.\n",
      "*   A comprehensive JSON schema for summarization is provided, incorporating dimensions like faithfulness, completeness, conciseness, coherence, fluency, and relevance, reflecting current research in text summarization evaluation.\n",
      "```\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google import genai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the Gemini API key\n",
    "gemini_key = os.getenv(\"GEMINI_KEY\")\n",
    "\n",
    "# Initialize the Gemini client\n",
    "client = genai.Client(api_key=gemini_key)\n",
    "\n",
    "# config = {\n",
    "#     \"temperature\": 0.7,\n",
    "#     \"max_output_tokens\": 250,\n",
    "#     \"top_p\": 0.8,\n",
    "#     \"top_k\": 20,\n",
    "#     # \"stop_sequences\": [\"```\n",
    "#     \"seed\": 42,\n",
    "#     # more parameters as needed\n",
    "# }\n",
    "\n",
    "\n",
    "config ={\n",
    "    \"temperature\":0.1,\n",
    "    \"top_p\":0.9,\n",
    "    \"top_k\":15,\n",
    "    # \"response_json_schema\": summary_schema,\n",
    "    # \"max_output_tokens\":300,\n",
    "    # \"presence_penalty\":0.2,\n",
    "    # \"frequency_penalty\":0.1,\n",
    "    # \"candidate_count\":1,\n",
    "    # \"stop_sequences\":[\"\\n\\n\"]\n",
    "}\n",
    "\n",
    "\n",
    "def summarize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes the input text using the Gemini model.\n",
    "    Args:\n",
    "        text (str): The text to be summarized.\n",
    "    Returns:\n",
    "        str: The summarized text wrapped in 'summary' tags.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate summary using Gemini model, instructing it to return the summary within 'summary' tags\n",
    "        prompt = f\"Summarize the following text try to get the major details and return the summary wrapped inside 'summary' tags:\\n\\n{text}\"\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemma-3-12b-it\",\n",
    "            contents=prompt,\n",
    "            config=config\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Ensure the response contains the summary\n",
    "        return f\"```\\n{response.text.strip()}\\n```\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        return \"Error occurred during summarization.\"\n",
    "\n",
    "# Example usage\n",
    "input_text = \"\"\"\n",
    "Home\n",
    "Finance\n",
    "Travel\n",
    "Academic\n",
    "Library\n",
    "is gemma 27b available through api? for free?\n",
    "is gemma 27b available through api? for free?\n",
    "from dotenv import load_dotenv import os from google import genai # Load environment variables loa\n",
    "from google import genai # The client gets the API key from the environment variable `GEMINI_API_KE\n",
    "import os import logging from google import genai from langchain_google_genai import ChatGoogleGener\n",
    "how do you debug a file if your python file uses venv, in vscode\n",
    "can I have dolby level sound in linux for free?\n",
    "what are the most popular austrilan shows\n",
    "best video player for linux\n",
    "download wifite in fedora\n",
    "which one of these are positive and which ones negative, mouse wire\n",
    "View All\n",
    "Home\n",
    "Discover\n",
    "Spaces\n",
    "Account\n",
    "\n",
    "Upgrade\n",
    "Install\n",
    "User avatar\n",
    "is gemma 27b available through api? for free?\n",
    "is gemma 27b available through api? for free?\n",
    "Availability of Gemma 27B via API and Free Access\n",
    "Key Takeaways:\n",
    "\n",
    "Yes, Google’s Gemma 3 27B (often referred to simply as Gemma 27B) is accessible through several hosted APIs.\n",
    "\n",
    "Free access is available via third-party gateways (OpenRouter and LangDB) under usage-limited tiers.\n",
    "\n",
    "Native Google offerings (Gemini API, Vertex AI) require valid API keys and are subject to billing beyond free trial quotas.\n",
    "\n",
    "1. Official Google APIs\n",
    "Gemini API\n",
    "Google’s Gemini API lets you invoke Gemma 3 27B programmatically.\n",
    "\n",
    "Endpoint: generativelanguage.googleapis.com/v1beta/models/gemma-3-27b-it:generateContent\n",
    "\n",
    "Authentication: API key (YOUR_API_KEY)\n",
    "\n",
    "Billing: standard Google Cloud rates apply; no perpetual free tier beyond initial credits.\n",
    "\n",
    "Google Cloud Vertex AI\n",
    "Via Vertex AI’s Model Garden, Gemma models (including 27B) can be deployed in your GCP project.\n",
    "\n",
    "Requires enabling Vertex AI and billing account.\n",
    "\n",
    "No ongoing free allowance for large-parameter inference; charges accrue per compute usage.\n",
    "\n",
    "2. Third-Party Gateways with Free Tiers\n",
    "OpenRouter.ai\n",
    "Model ID: google/gemma-3-27b-it:free\n",
    "\n",
    "Pricing: $0 / M input tokens, $0 / M output tokens\n",
    "\n",
    "Context window: 96K tokens\n",
    "\n",
    "Instant API key signup; usage subject to OpenRouter fair-use limits.\n",
    "\n",
    "LangDB AI Gateway\n",
    "Model ID: google/gemma-3-27b-it:free\n",
    "\n",
    "Free tier: available; no credit card required\n",
    "\n",
    "Monthly request cap: 10,000+\n",
    "\n",
    "Unified API interface for 250+ models, including Gemma 3 27B.\n",
    "\n",
    "3. Community-Hosted Endpoints\n",
    "Together AI\n",
    "Model: google/gemma-3-27b-it\n",
    "\n",
    "Requires deploying a dedicated endpoint and supplying a Together API key\n",
    "\n",
    "Paid plan only; no free tier.\n",
    "\n",
    "DeepInfra\n",
    "Model: google/gemma-2-27b-it (Gemma 2, predecessor)\n",
    "\n",
    "Paid access via DeepInfra token; no free requests by default.\n",
    "\n",
    "Hugging Face Inference API\n",
    "Weights available under Google’s license\n",
    "\n",
    "Must accept licensing terms and may require subscription for high-traffic usage\n",
    "\n",
    "No built-in free quota for large-parameter model inference.\n",
    "\n",
    "4. Self-Hosting\n",
    "You can also download the open weights for Gemma 27B from Hugging Face and run locally in any environment that supports a 27B-parameter model (e.g., multi-GPU or large-memory servers). This approach eliminates per-request fees but incurs infrastructure and operational costs.\n",
    "\n",
    "Conclusion\n",
    "Official Google APIs require billing beyond trial credits.\n",
    "\n",
    "Free API access is offered by OpenRouter.ai and LangDB under limited-usage tiers.\n",
    "\n",
    "For unlimited use, self-hosting the open weights is the only cost-predictable option (infrastructure costs only).\n",
    "\n",
    "how do you use gemini api with langchian latest doc\n",
    "how do you use gemini api with langchian latest doc\n",
    "Using the Gemini API with LangChain (Latest Version)\n",
    "Below is a step-by-step guide and a sample Python snippet showing how to integrate Google’s Gemini API (Gemma 3 27B) into a LangChain application using the latest LangChain releases.\n",
    "\n",
    "1. Prerequisites\n",
    "A Google Cloud project with the Generative AI (Gemini) API enabled\n",
    "\n",
    "A valid API key or service-account JSON credentials\n",
    "\n",
    "Python 3.9+\n",
    "\n",
    "Latest LangChain installed (pip install langchain google-generativeai)\n",
    "\n",
    "2. Install Required Packages\n",
    "bash\n",
    "pip install langchain google-generativeai\n",
    "3. Configure Credentials\n",
    "Option A: API Key\n",
    "Set the environment variable before running your script:\n",
    "\n",
    "bash\n",
    "export GOOGLE_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "Option B: Service Account\n",
    "Set the path to your JSON key file:\n",
    "\n",
    "bash\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/service-account.json\"\n",
    "4. Initialize the Gemini Model in LangChain\n",
    "python\n",
    "from langchain.llms import GoogleGemini\n",
    "from langchain import LLMChain, PromptTemplate, ConversationChain\n",
    "import os\n",
    "\n",
    "# Option A: Using API Key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Option B: Using Service Account (default application credentials)\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/your/service-account.json\"\n",
    "\n",
    "# Initialize the Gemini (Gemma 3 27B) model\n",
    "gemini_llm = GoogleGemini(\n",
    "    model_name=\"gemma-3-27b-it\",       # or \"gemma-3-27b\" for standard model\n",
    "    temperature=0.1,                   # adjust for creativity\n",
    "    max_output_tokens=512,             # output length limit\n",
    "    top_p=0.95,                        # nucleus sampling\n",
    ")\n",
    "\n",
    "# Example: Simple prompt completion\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the concept of {topic} in simple terms.\"\n",
    ")\n",
    "chain = LLMChain(llm=gemini_llm, prompt=prompt)\n",
    "\n",
    "response = chain.run({\"topic\": \"quantum entanglement\"})\n",
    "print(response)\n",
    "5. Building a Conversational Agent\n",
    "python\n",
    "from langchain import ConversationChain\n",
    "\n",
    "# Create a simple conversational chain\n",
    "conversation = ConversationChain(\n",
    "    llm=gemini_llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Simulate a back-and-forth dialogue\n",
    "print(conversation.predict(input=\"Hi, who are you?\"))\n",
    "print(conversation.predict(input=\"Can you summarize Gödel’s incompleteness theorems?\"))\n",
    "6. Advanced Usage\n",
    "Streaming responses: Enable by adding streaming=True and handling token callbacks.\n",
    "\n",
    "Custom callbacks: Inherit from LangChain’s BaseCallbackHandler to monitor tokens or latencies.\n",
    "\n",
    "Memory integration: Combine with ConversationBufferMemory to maintain context across turns.\n",
    "\n",
    "python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=gemini_llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "With these steps, you can seamlessly invoke the Gemini API within LangChain workflows—whether for single completions, chatbots, or complex chains.\n",
    "\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "grpcio-tools 1.74.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 5.29.5 which is incompatible.\n",
    "langchain-google-genai 2.1.8 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\n",
    "\n",
    "which lib to ditch with to use?\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. grpcio-tools 1.74.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 5.29.5 which is incompatible. langchain-google-genai 2.1.8 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.15 which is incompatible. which lib to ditch with to use?\n",
    "To resolve these conflicts, you’ll need to drop the langchain-google-genai bridge and use the standalone Google Generative AI client directly (the google-generativeai package), which aligns with the LangChain integration.\n",
    "\n",
    "Uninstall the conflicting bridge:\n",
    "\n",
    "bash\n",
    "pip uninstall langchain-google-genai\n",
    "Install a compatible version of the Generative AI client and protobuf:\n",
    "\n",
    "bash\n",
    "pip install google-generativeai>=0.6.18,<0.7.0 protobuf>=6.31.1\n",
    "In your code, switch from langchain.llms.GoogleGemini to using the helper in langchain.llms.GoogleGenerativeAI, e.g.:\n",
    "\n",
    "python\n",
    "from langchain.llms import GoogleGenerativeAI\n",
    "\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemma-3-27b-it\",\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=512,\n",
    ")\n",
    "This way you avoid the langchain-google-genai version constraints and stay compatible with both grpcio-tools and protobuf.\n",
    "\n",
    "so if I want to use gemma api with rag how can I do it with langchain. also give me the link to lastest doc\n",
    "so if I want to use gemma api with rag how can I do it with langchain. also give me the link to lastest doc\n",
    "To use the Gemma API (which is part of Google’s Gemini API family) with RAG (Retrieval Augmented Generation) in LangChain, you just need to:\n",
    "\n",
    "Obtain access to the Gemini API and have your endpoint information (API key/project/service account for Google Cloud).\n",
    "\n",
    "Use a LangChain LLM integration compatible with Gemma (such as via Vertex AI Model Garden or LangChain Google integrations).\n",
    "\n",
    "Build a RAG pipeline in LangChain, where the retriever pulls relevant docs and the LLM (Gemma via Gemini API) is used for generation.\n",
    "\n",
    "Example: Basic RAG with Gemma API in LangChain\n",
    "python\n",
    "# BEFORE YOU START: pip install langchain langchain-google-vertexai chromadb\n",
    "\n",
    "from langchain_google_vertexai import GemmaVertexAIModelGarden\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Set up your Gemini API credentials (service account or API key)\n",
    "# Example for service account credentials:\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/path/to/your/service-account.json'\n",
    "\n",
    "# 1. Load and split documents\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "documents = TextLoader('your_docs_folder/').load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# 2. Embed and index docs (using Google Palm embeddings)\n",
    "embeddings = GooglePalmEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# 3. Set up Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 4. Initialize Gemma via Vertex AI\n",
    "llm = GemmaVertexAIModelGarden(\n",
    "    endpoint_id=\"YOUR_VERTEX_AI_GEMMA_ENDPOINT_ID\",\n",
    "    project=\"YOUR_GCP_PROJECT\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# 5. Build RAG Pipeline\n",
    "prompt_template = \\\"\"\"Answer the following question based on the context:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\\\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 6. Ask a Question w/RAG\n",
    "query = \"What is retrieval augmented generation?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result[\"result\"])\n",
    "Key Notes:\n",
    "\n",
    "Replace endpoint/project/location with your actual Google Cloud setup.\n",
    "\n",
    "Swap in any other retriever (like pgvector, FAISS, etc.) as needed.\n",
    "\n",
    "You can use other LLM wrappers (GoogleGemini, etc.) as supported—see current integration details.\n",
    "\n",
    "Latest Documentation Links\n",
    "LangChain Google Integrations (Official provider docs):\n",
    "https://python.langchain.com/docs/integrations/providers/google/ \n",
    "\n",
    "Gemma + LangChain (Google’s Official Tutorial):\n",
    "https://ai.google.dev/gemma/docs/integrations/langchain \n",
    "\n",
    "Building a RAG App (LangChain’s Official RAG tutorial):\n",
    "https://python.langchain.com/docs/tutorials/rag/ \n",
    "\n",
    "These resources give up-to-date code and configuration for both basic and production RAG architectures using Google Gemini/Gemma APIs with LangChain.\n",
    "\n",
    "if I want gemma to have context before the chat, what is the best method to do it. if I want the ai to already know the context before I ask it the question. is that possible?\n",
    "\n",
    "and how can I use it with gemma model?\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv(\"GEMINI_KEY\")\n",
    "\n",
    "\n",
    "from google import genai\n",
    "\n",
    "\n",
    "client = genai.Client(api_key=gemini_key)\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "model=\"gemma-3-12b-it\",\n",
    "contents=\"who is a snake?\",\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)\n",
    "if I want gemma to have context before the chat, what is the best method to do it. if I want the ai to already know the context before I ask it the question. is that possible? and how can I use it with gemma model? from dotenv import load_dotenv import os load_dotenv() gemini_key = os.getenv(\"GEMINI_KEY\") from google import genai client = genai.Client(api_key=gemini_key) response = client.models.generate_content( model=\"gemma-3-12b-it\", contents=\"who is a snake?\", ) print(response.text)\n",
    "Yes, you can provide context to Gemma models (and Gemini API in general) before starting the chat—this is a core feature of modern large language models with sizeable context windows. There are two standard methods:\n",
    "\n",
    "1. Preload the Context as Part of the Initial Prompt\n",
    "You can prepend your knowledge/context as text before your question in the initial API call. For example, if you want the model to \"know\" something before any interaction:\n",
    "\n",
    "python\n",
    "context = \\\"\"\"\n",
    "You are an AI assistant specializing in snakes. Remember that a 'snake' is a legless reptile belonging to the suborder Serpentes. Use this definition when responding to questions about snakes. \n",
    "\\\"\"\"\n",
    "question = \"Who is a snake?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-12b-it\",\n",
    "    contents=context + \"\\n\" + question,\n",
    ")\n",
    "print(response.text)\n",
    "The model will take both your context and your question into account when generating the answer. This is the simplest and most universal method.\n",
    "\n",
    "2. Use Structured \"Contents\" for Multi-Turn Chat\n",
    "Google’s Gemini API (and similar frameworks) support multi-turn chat by using structured content/messages, just like conversation history:\n",
    "\n",
    "python\n",
    "context = \"A 'snake' is a legless reptile belonging to the suborder Serpentes.\"\n",
    "history = [\n",
    "    {\"role\": \"user\", \"parts\": [context]},\n",
    "    {\"role\": \"user\", \"parts\": [\"Who is a snake?\"]}\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-12b-it\",\n",
    "    contents=history,\n",
    ")\n",
    "print(response.text)\n",
    "This way, the model maintains awareness of context across turns.\n",
    "\n",
    "3. For Ongoing Chat or Multi-Turn Apps\n",
    "If you're building a chatbot or agent:\n",
    "\n",
    "Store the context (facts, rules, factsheet, persona, etc.) as the very first message(s) in your \"history\" list—with role \"system\" or as the first message(s) from the user.\n",
    "\n",
    "For each user query: always prepend/merge the context/history alongside the new question.\n",
    "\n",
    "LangChain and similar frameworks have chat memory buffers for exactly this.\n",
    "\n",
    "Key Points\n",
    "Gemma (via Gemini API) supports very large context windows (see docs for model-specific limits, e.g. 128K+ tokens).\n",
    "\n",
    "Any information at the start of the conversation will be considered context—there’s no technical difference between \"background context\" and a \"prompt\", as long as it fits within the context window.\n",
    "\n",
    "For the best results, explicitly insert all necessary context each time you call the API.\n",
    "\n",
    "References and Docs\n",
    "Long context windows in Gemini/Gemma\n",
    "\n",
    "Gemma+LangChain multi-turn chat docs\n",
    "\n",
    "Contextual information for Vertex AI / Gemini\n",
    "\n",
    "These documents explain how to build context-rich prompts and multi-turn systems with Gemma.\n",
    "\n",
    "like if I want to create a summarizer with gemini. ask it to return the summary inside ```summary``` or something so that I could detect it\n",
    "like if I want to create a summarizer with gemini. ask it to return the summary inside ```summary``` or something so that I could detect it\n",
    "You can instruct Gemini (Gemma) explicitly in your prompt to return the summary wrapped inside special delimiters like triple backticks (summary\n",
    "\n",
    "Here's how you can do it:\n",
    "\n",
    "python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv(\"GEMINI_KEY\")\n",
    "\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=gemini_key)\n",
    "\n",
    "text_to_summarize = \\\"\"\"\n",
    "Your long input text goes here. It can be several paragraphs or an article to summarize.\n",
    "\\\"\"\"\n",
    "\n",
    "prompt = f\\\"\"\"\n",
    "Please provide a concise summary of the following text.\n",
    "Return **ONLY** the summary, wrapped exactly like this:\n",
    "\n",
    "<Your summary here> \\```\n",
    "Text:\n",
    "{text_to_summarize}\n",
    "\\\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "model=\"gemma-3-12b-it\",\n",
    "contents=prompt,\n",
    ")\n",
    "\n",
    "print(response.text) # The summary will be wrapped as requested\n",
    "\n",
    "text\n",
    "\n",
    "## Why this works:\n",
    "\n",
    "- The model follows explicit instructions to format the output inside the backticks labeled `summary`.\n",
    "- You can then parse `response.text` in your code to extract the text between ```summary```\n",
    "- This technique is a common prompt engineering pattern for structured outputs.\n",
    "\n",
    "If you want to integrate this in a LangChain flow, define a prompt template with these instructions and parse outputs accordingly.\n",
    "\n",
    "This approach enables easy automated consumption of summaries or any formatted outputs from Gemini/Gemma models.\n",
    "what are the methods to use the api? like we can controll the temp and stuff what are the other methods list them all, like search for the latest method that is valid as of today.\n",
    "what are the methods to use the api? like we can controll the temp and stuff what are the other methods list them all, like search for the latest method that is valid as of today.\n",
    "Here are the latest valid methods and parameters you can use with Google Gemini/Gemma API’s generate_content (as of August 2025). These apply to both the Python SDK and direct API calls:\n",
    "\n",
    "Core Parameters/Methods for Generation\n",
    "Within the config argument (Python SDK) or JSON payload:\n",
    "\n",
    "Parameter\tWhat it Does\tTypical Range/Type\n",
    "temperature\tControls randomness/creativity. Lower = deterministic, higher = more creative.\t0.0–2.0 (float)\n",
    "top_p\tNucleus sampling. Limits choices to top-probable tokens summing to this value.\t0.0–1.0 (float)\n",
    "top_k\tOnly considers the top K most likely tokens for each generation step.\t1–40 (int)\n",
    "max_output_tokens\tSets maximum response length in tokens.\t1–8192+ (int, varies)\n",
    "stop_sequences\tArray of strings; model stops output when any is generated.\t[ \"STOP!\", ... ]\n",
    "presence_penalty\tPenalizes repeating tokens/words found anywhere in the generated text.\t-2.0–2.0 (float)\n",
    "frequency_penalty\tPenalizes repeating tokens/words more than others.\t-2.0–2.0 (float)\n",
    "seed\tFor reproducibility. Fixes the pseudo-random generator; same prompt/params = same result.\tint\n",
    "candidate_count\tNumber of completions to return for the same prompt.\t1–n (int)\n",
    "response_mime_type\tReturn as \"text/plain\", \"application/json\", etc. For structured output or code/execution.\tstring\n",
    "response_schema\tSpecifies expected structure/schema of output (for JSON mode, etc.).\tJSON Schema\n",
    "logprobs\tReturns log probabilities of top candidate tokens (for debugging/output analysis).\tint (1–20)\n",
    "response_logprobs\tWhether to return logprobs for output tokens as well.\ttrue/false\n",
    "Additional Functionalities\n",
    "System Instructions: Set system behavior/persona.\n",
    "\n",
    "Multimodal Inputs: Supply files (images, audio, PDF, video).\n",
    "\n",
    "Function Calling: Provide tool/plugin spec for function calling mode.\n",
    "\n",
    "Streaming: Use generate_content_stream to receive output tokens as they’re generated.\n",
    "\n",
    "Chat History: Provide a list of previous Content objects (with \"role\": \"user\" or \"model\") for multi-turn chat.\n",
    "\n",
    "Caching: Reuse pre-analyzed files/chunks via content cache APIs.\n",
    "\n",
    "Python Example\n",
    "python\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "client = genai.Client(api_key=gemini_key)\n",
    "\n",
    "config = GenerateContentConfig(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    max_output_tokens=512,\n",
    "    stop_sequences=[\"```\n",
    "    presence_penalty=0.3,\n",
    "    frequency_penalty=0.1,\n",
    "    seed=42,\n",
    "    candidate_count=1,\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-27b-it\",\n",
    "    contents=\"Summarize the following article...\",\n",
    "    config=config\n",
    ")\n",
    "print(response.text)\n",
    "Official Documentation\n",
    "For the most detailed and updated API reference, see:\n",
    "\n",
    "Gemini API: Generating Content (all parameters, multimodal, tuning, examples)\n",
    "\n",
    "Vertex AI: Content Generation Parameters Table & Descriptions\n",
    "\n",
    "Tip:\n",
    "Parameters available can differ per model size/type. Always check model-specific docs or the Python SDK reference for latest, supported values and options.\n",
    "\n",
    "config = GenerateContentConfig(\n",
    "temperature=0.7,\n",
    "top_p=0.95,\n",
    "top_k=20,\n",
    "max_output_tokens=512,\n",
    "stop_sequences=[\"```\n",
    "presence_penalty=0.3,\n",
    "frequency_penalty=0.1,\n",
    "seed=42,\n",
    "candidate_count=1,\n",
    ")\n",
    "\n",
    "\n",
    "do you know the meaning of all of this?\n",
    "config = GenerateContentConfig( temperature=0.7, top_p=0.95, top_k=20, max_output_tokens=512, stop_sequences=[\"``` presence_penalty=0.3, frequency_penalty=0.1, seed=42, candidate_count=1, ) do you know the meaning of all of this?\n",
    "Absolutely! Here’s what each parameter in your GenerateContentConfig means and how it affects the model’s output:\n",
    "\n",
    "Parameter Meanings\n",
    "temperature=0.7\n",
    "Controls randomness in the output. Lower (e.g., 0.2) makes responses more predictable and conservative; higher (closer to 1 or 2) can make them more creative and diverse.\n",
    "\n",
    "top_p=0.95\n",
    "Nucleus sampling: the model considers only the smallest set of tokens whose total probability exceeds this value. 0.95 is a typical setting for balanced diversity without too much randomness.\n",
    "\n",
    "top_k=20\n",
    "At each generation step, limits the set of possible next tokens to the top 20 highest-probability options. Lower values = safer, higher values = more variety.\n",
    "\n",
    "max_output_tokens=512\n",
    "The maximum number of tokens (words and pieces of words) the output can contain. Prevents runaway output.\n",
    "\n",
    "**stop_sequences=[\"** Tells the model to stop generating once it outputs the string\n",
    "\n",
    "presence_penalty=0.3\n",
    "Discourages the model from repeating tokens that have already appeared anywhere in the generated text. Mild penalty here slightly lowers repetition.\n",
    "\n",
    "frequency_penalty=0.1\n",
    "Penalizes repeating the same tokens within the text, especially those used most often. Helps prevent word overuse.\n",
    "\n",
    "seed=42\n",
    "Fixes the “randomness” for reproducible results: using the same prompt and config with the same seed always gives the same output.\n",
    "\n",
    "candidate_count=1\n",
    "Specifies that you only want one completion per API call. For multiple completions (alternative outputs), set this to a higher integer.\n",
    "\n",
    "Summary:\n",
    "These parameters let you finely tune the behavior, style, and reliability of the model’s responses—making output anywhere from highly factual and consistent to creative and varied, depending on your use case. Each one is a common lever for prompt engineers and AI developers working with generative models.\n",
    "\n",
    "so I think most of the values should be default no point in chaning them? right? but some you need to change depending on the context. my context is to create a summarizer. means it will summarize the text with best accuracy. also by caring about the speed. what the values should be?\n",
    "so I think most of the values should be default no point in chaning them? right? but some you need to change depending on the context. my context is to create a summarizer. means it will summarize the text with best accuracy. also by caring about the speed. what the values should be?\n",
    "For building a summarizer using Gemma with a focus on best accuracy and reasonable speed, you generally want to keep most parameters near default but adjust a few key ones based on your goals:\n",
    "\n",
    "Recommended Parameter Settings for Summarization\n",
    "Parameter\tSuggested Value\tReason/Effect\n",
    "temperature\t0.0 – 0.3\tLow temperature reduces randomness, making the summary more focused and consistent.\n",
    "top_p\t0.8 – 0.95\tTypical values that keep outputs coherent while allowing slight creativity if needed.\n",
    "top_k\t10 – 20\tLimits token choices to top 10–20 to maintain quality without hurting speed much.\n",
    "max_output_tokens\tDepends on summary length, e.g. 256 – 512\tSet reasonably to cover summary length without wasting tokens on unnecessary length.\n",
    "stop_sequences\tOptional, e.g., [\"\\n\\n\"] or custom\tTo stop generation after summary ends. Helps avoid overly long or runaway outputs.\n",
    "presence_penalty\t0.0 – 0.3\tMild penalty to avoid needless repetition in summaries.\n",
    "frequency_penalty\t0.0 – 0.2\tSlight penalty discourages repeated word use, improving summary clarity.\n",
    "seed\tNone or fixed (optional)\tFix if you want reproducible results during testing, otherwise omit for variety.\n",
    "candidate_count\t1\tUsually one summary is enough. Can increase if you want multiple variants to choose from.\n",
    "Why these values?\n",
    "Low temperature ensures the summary is factual, not creative or “storytelling.”\n",
    "\n",
    "Controlled top_p/top_k balances coherence and speed.\n",
    "\n",
    "Summaries benefit from not being overly verbose, so token limits are moderate.\n",
    "\n",
    "Stop sequences can help APIs halt generation early for speed.\n",
    "\n",
    "Mild penalties help reduce needless repetition common in automatic summaries.\n",
    "\n",
    "Example Config for Summarizer\n",
    "python\n",
    "config = GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    top_k=15,\n",
    "    max_output_tokens=300,\n",
    "    presence_penalty=0.2,\n",
    "    frequency_penalty=0.1,\n",
    "    candidate_count=1,\n",
    "    stop_sequences=[\"\\n\\n\"]\n",
    ")\n",
    "Additional Tips\n",
    "Use streaming generation if supported, to see partial summaries faster.\n",
    "\n",
    "Tune max_output_tokens based on your average input length and desired summary length.\n",
    "\n",
    "You can experiment with stop_sequences and adjust it to detect the end of your summary reliably.\n",
    "\n",
    "This configuration should give you accurate, concise summaries while keeping latency reasonable for API calls.\n",
    "\n",
    "user_profile = {\n",
    "'properties': {\n",
    "'age': {\n",
    "'anyOf': [\n",
    "{'maximum': 20, 'minimum': 0, 'type': 'integer'},\n",
    "{'type': 'null'},\n",
    "],\n",
    "'title': 'Age',\n",
    "},\n",
    "'username': {\n",
    "'description': \"User's unique name\",\n",
    "'title': 'Username',\n",
    "'type': 'string',\n",
    "},\n",
    "},\n",
    "'required': ['username', 'age'],\n",
    "'title': 'User Schema',\n",
    "'type': 'object',\n",
    "}\n",
    "\n",
    "response = client.models.generate_content(\n",
    "model='gemini-2.0-flash',\n",
    "contents='Give me information of the United States.',\n",
    "config={\n",
    "'response_mime_type': 'application/json',\n",
    "'response_json_schema': userProfile\n",
    "},\n",
    ")\n",
    "print(response.parsed)\n",
    "use this style on text summarization, so that the llm will return json format\n",
    "user_profile = { 'properties': { 'age': { 'anyOf': [ {'maximum': 20, 'minimum': 0, 'type': 'integer'}, {'type': 'null'}, ], 'title': 'Age', }, 'username': { 'description': \"User's unique name\", 'title': 'Username', 'type': 'string', }, }, 'required': ['username', 'age'], 'title': 'User Schema', 'type': 'object', } response = client.models.generate_content( model='gemini-2.0-flash', contents='Give me information of the United States.', config={ 'response_mime_type': 'application/json', 'response_json_schema': userProfile }, ) print(response.parsed) use this style on text summarization, so that the llm will return json format\n",
    "To have the Gemini API return a summary in JSON format using a schema (similar to your example with user_profile), you need to define a JSON schema for the summary and pass it via the response_json_schema parameter in the config, along with setting response_mime_type to \"application/json\".\n",
    "\n",
    "Here’s how you can adapt your code for text summarization returning JSON:\n",
    "\n",
    "Step 1: Define a JSON schema for the summary output\n",
    "For example, a simple summary schema might look like this:\n",
    "\n",
    "python\n",
    "summary_schema = {\n",
    "    \"title\": \"Summary Schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the input text\"\n",
    "        },\n",
    "        \"length\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"The number of words in the summary\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "Step 2: Call the API with JSON response config\n",
    "python\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-12b-it\",\n",
    "    contents=f\"Please provide a concise summary of the following text:\\n\\n{text_to_summarize}\",\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": summary_schema,\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_output_tokens\": 300,\n",
    "    },\n",
    ")\n",
    "print(response.parsed)\n",
    "Explanation:\n",
    "response_mime_type=\"application/json\" tells the API you want JSON output.\n",
    "\n",
    "response_json_schema provides a strict schema the model’s output should follow. The model tries to validate its response against this schema.\n",
    "\n",
    "response.parsed will contain the parsed JSON object matching the schema (e.g., a dictionary with \"summary\" and optionally \"length\").\n",
    "\n",
    "Why use this approach?\n",
    "Enforces structured output suitable for downstream automatic processing.\n",
    "\n",
    "Avoids tedious string parsing or regex to extract the summary.\n",
    "\n",
    "Makes integrating Gemini API with applications easier and safer.\n",
    "\n",
    "If needed, you can expand the schema with more fields (e.g., keywords, confidence_score) for richer summaries.\n",
    "\n",
    "This method leverages Gemini’s powerful schema-guided generation feature to get clean, machine-readable responses for summarization tasks.\n",
    "\n",
    "summary_schema = {\n",
    "\"title\": \"Summary Schema\",\n",
    "\"type\": \"object\",\n",
    "\"properties\": {\n",
    "\"summary\": {\n",
    "\"type\": \"string\",\n",
    "\"description\": \"A concise summary of the input text\"\n",
    "},\n",
    "\"length\": {\n",
    "\"type\": \"integer\",\n",
    "\"description\": \"The number of words in the summary\"\n",
    "}\n",
    "},\n",
    "\"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "\n",
    "create the perfect scehema for summarization with latest research, 2025 latest\n",
    "summary_schema = { \"title\": \"Summary Schema\", \"type\": \"object\", \"properties\": { \"summary\": { \"type\": \"string\", \"description\": \"A concise summary of the input text\" }, \"length\": { \"type\": \"integer\", \"description\": \"The number of words in the summary\" } }, \"required\": [\"summary\"] } create the perfect scehema for summarization with latest research, 2025 latest\n",
    "Based on the latest 2025 research in text summarization evaluation, here's a comprehensive schema that incorporates cutting-edge assessment dimensions and metrics:\n",
    "\n",
    "python\n",
    "summary_schema = {\n",
    "    \"title\": \"Advanced Summary Evaluation Schema 2025\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The concise summary of the input text\"\n",
    "        },\n",
    "        \"evaluation_dimensions\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"faithfulness\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"factual_consistency_score\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"Alignment between summary and source text facts (1-5 scale)\"\n",
    "                        },\n",
    "                        \"hallucination_count\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"minimum\": 0,\n",
    "                            \"description\": \"Number of factual inconsistencies or hallucinated facts\"\n",
    "                        },\n",
    "                        \"contradiction_detected\": {\n",
    "                            \"type\": \"boolean\",\n",
    "                            \"description\": \"Whether summary contradicts source information\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"factual_consistency_score\", \"hallucination_count\"]\n",
    "                },\n",
    "                \"completeness\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"coverage_score\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"How well summary covers key information (1-5 scale)\"\n",
    "                        },\n",
    "                        \"key_facts_captured\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"minimum\": 0,\n",
    "                            \"description\": \"Number of essential facts included from source\"\n",
    "                        },\n",
    "                        \"missing_critical_info\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\"type\": \"string\"},\n",
    "                            \"description\": \"List of important information omitted\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"coverage_score\", \"key_facts_captured\"]\n",
    "                },\n",
    "                \"conciseness\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"compression_ratio\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"description\": \"Ratio of summary length to source length (0-1)\"\n",
    "                        },\n",
    "                        \"redundancy_score\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"Level of redundant information (1=high redundancy, 5=no redundancy)\"\n",
    "                        },\n",
    "                        \"information_density\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"Amount of useful information per word (1-5 scale)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"compression_ratio\", \"redundancy_score\"]\n",
    "                },\n",
    "                \"coherence\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"logical_flow_score\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"Logical organization and flow of ideas (1-5 scale)\"\n",
    "                        },\n",
    "                        \"structural_quality\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"Well-structured and organized content (1-5 scale)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"logical_flow_score\"]\n",
    "                },\n",
    "                \"fluency\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"grammatical_correctness\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 3.0,\n",
    "                            \"description\": \"Grammar and syntax quality (1-3 scale)\"\n",
    "                        },\n",
    "                        \"readability_score\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"Ease of reading and understanding (1-5 scale)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"grammatical_correctness\"]\n",
    "                },\n",
    "                \"relevance\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"topic_alignment_score\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"How well summary aligns with main topics (1-5 scale)\"\n",
    "                        },\n",
    "                        \"importance_prioritization\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 5.0,\n",
    "                            \"description\": \"Focus on most important information (1-5 scale)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"topic_alignment_score\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"faithfulness\", \"completeness\", \"conciseness\", \"coherence\", \"fluency\", \"relevance\"]\n",
    "        },\n",
    "        \"meta_analysis\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"overall_quality_score\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 5.0,\n",
    "                    \"description\": \"Holistic quality assessment (1-5 scale)\"\n",
    "                },\n",
    "                \"word_count\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"description\": \"Number of words in the summary\"\n",
    "                },\n",
    "                \"sentence_count\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"description\": \"Number of sentences in the summary\"\n",
    "                },\n",
    "                \"extractiveness_ratio\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"description\": \"Proportion of text directly extracted vs. abstractive (0=fully abstractive, 1=fully extractive)\"\n",
    "                },\n",
    "                \"confidence_level\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"description\": \"AI confidence in the summary quality assessment\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"overall_quality_score\", \"word_count\", \"sentence_count\"]\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\", \"evaluation_dimensions\", \"meta_analysis\"]\n",
    "}\n",
    "Key Features of This 2025 Schema:\n",
    "Multi-Dimensional Assessment: Based on latest research identifying 6 core dimensions: faithfulness, completeness, conciseness, coherence, fluency, and relevance.\n",
    "\n",
    "Fine-Grained Evaluation: Incorporates sentence-level and fact-level analysis as recommended by recent studies.\n",
    "\n",
    "Hallucination Detection: Explicitly tracks factual inconsistencies and contradictions.\n",
    "\n",
    "Coverage Analysis: Measures both what's included and what's missing from key information.\n",
    "\n",
    "Semantic Quality: Goes beyond surface-level metrics to assess meaning and information density.\n",
    "\n",
    "Confidence Scoring: Includes AI self-assessment of evaluation reliability.\n",
    "\n",
    "This schema aligns with the MSumBench and FineSurE frameworks from 2025 research, providing comprehensive evaluation that correlates better with human judgment than traditional metrics like ROUGE.\n",
    "\n",
    "\"\"\"\n",
    "summarized_text = summarize_text(input_text)\n",
    "# print(f\"Original Text:\\n{input_text}\")\n",
    "print(f\"Summarized Text:\\n{summarized_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a269ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e6e09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down who a \"snake\" is. Here's a comprehensive look, covering the biological, cultural, and symbolic aspects:\n",
      "\n",
      "**1. Biologically: What is a Snake?**\n",
      "\n",
      "*   **Scientific Classification:** Snakes are reptiles belonging to the suborder *Serpentes*. They are part of the larger order *Squamata*, which also includes lizards.\n",
      "*   **Key Characteristics:**\n",
      "    *   **Elongated, Legless Body:** This is the most defining feature. Snakes evolved from lizards and lost their limbs over millions of years.\n",
      "    *   **Scales:** Their bodies are covered in scales made of keratin (the same material as our fingernails). These scales provide protection and help with movement.\n",
      "    *   **Flexible Jaws:** Snakes have incredibly flexible jaws that allow them to swallow prey much larger than their heads.  The bones in their jaws are loosely connected, and they have a ligament that allows the jaw to stretch.\n",
      "    *   **Forked Tongue:**  Snakes use their forked tongue to \"taste\" the air and ground, gathering scent particles.  These particles are transferred to the Jacobson's organ (vomeronasal organ) in the roof of their mouth, which helps them identify prey, predators, and potential mates.\n",
      "    *   **No External Ears:** Snakes don't have external ears, but they can still detect vibrations through their jawbone and body.\n",
      "    *   **Variety of Sizes:** Snakes range dramatically in size.  The smallest snake, the Barbados threadsnake, is only about 4 inches (10 cm) long. The largest, the reticulated python, can grow over 20 feet (6 meters) long.\n",
      "*   **Types of Snakes:** There are over 3,900 species of snakes found worldwide (except for Antarctica). They are broadly categorized into:\n",
      "    *   **Non-Venomous Snakes:**  These snakes kill their prey by constriction (squeezing), swallowing whole, or simply overpowering them. Examples: pythons, boas, garter snakes, rat snakes.\n",
      "    *   **Venomous Snakes:** These snakes inject venom to subdue or kill their prey.  Venomous snakes are further divided into categories based on their venom delivery systems (fangs):\n",
      "        *   **Viperids:**  Have hinged fangs that fold back when not in use (e.g., rattlesnakes, cobras, vipers).\n",
      "        *   **Elapids:** Have fixed, hollow fangs (e.g., cobras, mambas, coral snakes).\n",
      "        *   **Colubrids:** A large and diverse group, some of which are mildly venomous, and some are non-venomous.\n",
      "\n",
      "**2. Cultural Significance & Symbolism:**\n",
      "\n",
      "Snakes have held profound symbolic meaning in cultures around the world for millennia.  Here are some common interpretations:\n",
      "\n",
      "*   **Transformation & Renewal:**  Snakes shed their skin, which has led to associations with rebirth, transformation, and healing.  The cyclical nature of shedding mirrors the cycles of life, death, and renewal.\n",
      "*   **Healing & Medicine:** The Rod of Asclepius (a snake coiled around a staff) is a symbol of medicine and healing in many Western cultures.  This originates from Greek mythology, where Asclepius was a god of healing.\n",
      "*   **Evil & Temptation:** In some cultures (particularly influenced by Abrahamic religions), snakes are associated with evil, temptation, and deceit (e.g., the serpent in the Garden of Eden).\n",
      "*   **Wisdom & Knowledge:** In some Eastern traditions, snakes are seen as symbols of wisdom, knowledge, and spiritual power.\n",
      "*   **Fertility & Creation:**  In some cultures, snakes are linked to fertility and the creation of the world.\n",
      "*   **Fear & Danger:**  Due to the venomous nature of some snakes, they are often associated with fear, danger, and the unknown.\n",
      "\n",
      "**3. Where Snakes Live:**\n",
      "\n",
      "*   Snakes are found on every continent except Antarctica.\n",
      "*   They inhabit a wide range of habitats, including:\n",
      "    *   Forests\n",
      "    *   Deserts\n",
      "    *   Grasslands\n",
      "    *   Swamps\n",
      "    *   Oceans (sea snakes)\n",
      "    *   Mountains\n",
      "\n",
      "\n",
      "\n",
      "**To help me tailor my answer further, could you tell me:**\n",
      "\n",
      "*   **What prompted you to ask \"who is a snake?\"** Are you curious about the biology, the symbolism, or something else?\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv(\"GEMINI_KEY\")\n",
    "\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=gemini_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-12b-it\",\n",
    "    contents=\"who is a snake?\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41d53fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down \"who is a snake?\" This can be interpreted in a few ways, so I'll cover them all!\n",
      "\n",
      "**1. Biologically: What *is* a snake?**\n",
      "\n",
      "*   **Reptiles:** Snakes are reptiles. This means they are cold-blooded (ectothermic), have scales, and typically lay eggs (though some give birth to live young).\n",
      "*   **Elongated, Legless:** The defining characteristic of snakes is their long, cylindrical body and lack of legs (though some primitive snakes have vestigial pelvic bones, remnants of legs their ancestors had).\n",
      "*   **Carnivorous:**  Snakes are predators. They eat other animals – mammals, birds, reptiles, amphibians, fish, and even other snakes!  They swallow their prey whole.\n",
      "*   **Diverse:** There are over 3,900 species of snakes found on every continent except Antarctica. They come in a huge range of sizes, colors, and habitats.\n",
      "*   **Examples:**  Common examples include:\n",
      "    *   **Cobras:** Known for their hood.\n",
      "    *   **Pythons & Boas:** Large constrictors.\n",
      "    *   **Rattlesnakes:**  Have a rattle on their tail.\n",
      "    *   **Garter Snakes:**  Common, non-venomous snakes.\n",
      "    *   **Coral Snakes:**  Brightly colored and venomous.\n",
      "\n",
      "\n",
      "\n",
      "**2. Figuratively: \"Snake\" as a negative term for a person**\n",
      "\n",
      "*   **Treacherous/Deceitful:**  Calling someone a \"snake\" is a very strong insult. It means you believe they are untrustworthy, deceitful, and likely to betray you.  It implies they are sly and cunning.\n",
      "*   **Backstabber:** A \"snake\" might be someone who talks behind your back, spreads rumors, or undermines you secretly.\n",
      "*   **Hidden Danger:** The association comes from the snake's reputation for being hidden and delivering a sudden, venomous bite.  A \"snake\" as a person is seen as someone who seems harmless but is actually dangerous.\n",
      "*   **Origin of the association:**  The negative connotation of snakes dates back to ancient mythology and religion (see below).\n",
      "\n",
      "**3.  Mythologically/Culturally: Snakes in symbolism**\n",
      "\n",
      "*   **Ancient Symbolism:** Snakes have held powerful symbolic meaning in many cultures throughout history.\n",
      "*   **Good and Evil:**  Snakes can represent both good and evil.\n",
      "    *   **Healing/Rebirth:**  In some cultures (like ancient Greece, with the Rod of Asclepius), snakes are associated with healing, medicine, and rebirth because they shed their skin.\n",
      "    *   **Chaos/Evil:** In other cultures (like the Judeo-Christian tradition), snakes are often seen as symbols of temptation, evil, and chaos (think of the serpent in the Garden of Eden).\n",
      "*   **Fertility/Creation:**  In some cultures, snakes represent fertility and creative life force.\n",
      "*   **Power/Royalty:**  Snakes have been used as symbols of power and royalty in various cultures (e.g., the Uraeus cobra on the crowns of Egyptian pharaohs).\n",
      "*   **Transformation:** Because they shed their skin, snakes are often seen as symbols of transformation and renewal.\n",
      "\n",
      "\n",
      "\n",
      "**4. Specific People Nicknamed \"Snake\"**\n",
      "\n",
      "*   **\"Snake\" Plissken:**  A famous character played by Kurt Russell in the movies *Escape from New York* and *Escape from L.A.* He's a cynical, anti-heroic outlaw.\n",
      "*   **Various Athletes:**  The nickname \"Snake\" is sometimes used for athletes, often those known for being quick or elusive.\n",
      "\n",
      "\n",
      "\n",
      "**To help me give you a more focused answer, could you tell me:**\n",
      "\n",
      "*   **What context are you asking about?** (e.g., Are you asking about animals? Are you talking about a person you know? Are you interested in mythology?)\n",
      "*   **What made you ask this question?** (Knowing the reason behind your question will help me understand what you're looking for.)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv(\"GEMINI_KEY\")\n",
    "search_engine_id = os.getenv(\"SEARCH_ENGINE_ID\")\n",
    "\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=gemini_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-27b-it\",\n",
    "    contents=\"who is a snake?\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9bb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# Ensure your API key is set:\n",
    "# export GEMINI_API_KEY=\"YOUR_API_KEY\"\n",
    "\n",
    "client = genai.Client(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-27b-it\",       # the hosted Gemma 3 27B endpoint\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    "    temperature=0.7,              # optional: control randomness\n",
    "    max_tokens=256                # optional: limit response length\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81142529",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Make sure you’ve set your GEMINI_API_KEY environment variable:\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# export GEMINI_API_KEY=\"YOUR_API_KEY_HERE\"\u001b[39;00m\n\u001b[32m      6\u001b[39m client = genai.Client()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemma-3-27b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# use the 27 B Gemma model\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain how AI works in a few words\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# temperature=0.7,            # optional: adjust creativity\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_tokens=256              # optional: limit response length\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/models.py:5835\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5833\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5834\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5835\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5836\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5837\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5838\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5839\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/models.py:4761\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4758\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4759\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4761\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4762\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4763\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4765\u001b[39m response_dict = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.body \u001b[38;5;28;01melse\u001b[39;00m json.loads(response.body)\n\u001b[32m   4767\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/_api_client.py:1177\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1169\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1172\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1173\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1174\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1175\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1176\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1178\u001b[39m   response_body = (\n\u001b[32m   1179\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1180\u001b[39m   )\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/_api_client.py:997\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m    994\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:418\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    416\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:185\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/_api_client.py:974\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    968\u001b[39m       method=http_request.method,\n\u001b[32m    969\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    972\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    973\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    976\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    977\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/errors.py:105\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    103\u001b[39m status_code = response.status_code\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    107\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# Make sure you’ve set your GEMINI_API_KEY environment variable:\n",
    "# export GEMINI_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-27b\",        # use the 27 B Gemma model\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    "    # temperature=0.7,            # optional: adjust creativity\n",
    "    # max_tokens=256              # optional: limit response length\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92437801",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'methods'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Print out each model’s name and supported methods\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: supports \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethods\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Model' object has no attribute 'methods'"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# Ensure GEMINI_API_KEY is set in your environment\n",
    "client = genai.Client()\n",
    "\n",
    "# List all models available for generateContent\n",
    "models = client.models.list()\n",
    "\n",
    "# Print out each model’s name and supported methods\n",
    "for m in models:\n",
    "    print(f\"{m.name}: supports {m.methods}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008731ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 10:35:45,663 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-08-05 10:35:57,768 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 401 Unauthorized\"\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "401 UNAUTHENTICATED. {'error': {'code': 401, 'message': 'API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication', 'status': 'UNAUTHENTICATED', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'CREDENTIALS_MISSING', 'domain': 'googleapis.com', 'metadata': {'method': 'google.cloud.aiplatform.v1.PredictionService.GenerateContent', 'service': 'aiplatform.googleapis.com'}}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initialize client with Vertex AI integration\u001b[39;00m\n\u001b[32m      6\u001b[39m client = genai.Client(\n\u001b[32m      7\u001b[39m     http_options=HttpOptions(api_version=\u001b[33m\"\u001b[39m\u001b[33mv1\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      8\u001b[39m     vertexai=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m     api_key=api_key\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain the concept of quantum entanglement.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/models.py:5835\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5833\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5834\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5835\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5836\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5837\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5838\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5839\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/models.py:4761\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4758\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4759\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4761\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4762\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4763\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4765\u001b[39m response_dict = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.body \u001b[38;5;28;01melse\u001b[39;00m json.loads(response.body)\n\u001b[32m   4767\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/_api_client.py:1177\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1169\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1172\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1173\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1174\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1175\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1176\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1178\u001b[39m   response_body = (\n\u001b[32m   1179\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1180\u001b[39m   )\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/_api_client.py:997\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m    994\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:418\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    416\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:185\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/_api_client.py:974\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    968\u001b[39m       method=http_request.method,\n\u001b[32m    969\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    972\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    973\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    976\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    977\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/ShareDrive/pytorch_env/lib64/python3.13/site-packages/google/genai/errors.py:105\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    103\u001b[39m status_code = response.status_code\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    107\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 401 UNAUTHENTICATED. {'error': {'code': 401, 'message': 'API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication', 'status': 'UNAUTHENTICATED', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'CREDENTIALS_MISSING', 'domain': 'googleapis.com', 'metadata': {'method': 'google.cloud.aiplatform.v1.PredictionService.GenerateContent', 'service': 'aiplatform.googleapis.com'}}]}}"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "import os\n",
    "\n",
    "# Initialize client with Vertex AI integration\n",
    "client = genai.Client(\n",
    "    http_options=HttpOptions(api_version=\"v1\"),\n",
    "    vertexai=True,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain the concept of quantum entanglement.\"\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bd098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
